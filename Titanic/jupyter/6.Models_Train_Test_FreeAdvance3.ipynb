{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6716a22e-3256-4b4f-8ad4-0185b2f9d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.데이터 로드\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "X_train = train.drop(columns='Survived')\n",
    "y_train = train['Survived']\n",
    "\n",
    "X_test = pd.read_csv('data/test.csv')\n",
    "PassengerId = X_test['PassengerId'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edf30907-db6c-4de8-b310-feb39838b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.EDA\n",
    "import seaborn as sns\n",
    "\n",
    "def graph(variable):\n",
    "    survived = train[train['Survived']==1][variable].value_counts()\n",
    "    dead = train[train['Survived']==0][variable].value_counts()\n",
    "    result = pd.DataFrame([survived,dead])\n",
    "    result.index = ['Survived','Dead']\n",
    "    result.T.plot(kind='bar', color=['skyblue','gray'], rot=0, figsize=(8,6))\n",
    "\n",
    "#graph('Parch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21f5c3aa-df94-4a51-aa7c-f24e3f6c66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.데이터 정제\n",
    "#print(X_train['Fare'].unique())\n",
    "#print(X_train.select_dtypes('object').nunique())\n",
    "\n",
    "#불필요한 컬럼 제거\n",
    "X_train = X_train.drop(columns=['PassengerId','Name','Ticket','Cabin'])\n",
    "X_test = X_test.drop(columns=['PassengerId','Name','Ticket','Cabin'])\n",
    "\n",
    "#SibSp, Parch 컬럼 통합\n",
    "X_train['Family'] = X_train['SibSp']+X_train['Parch']\n",
    "X_train = X_train.drop(columns=['SibSp','Parch'])\n",
    "\n",
    "X_test['Family'] = X_test['SibSp']+X_test['Parch']\n",
    "X_test = X_test.drop(columns=['SibSp','Parch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84d32119-819b-4924-9c25-2ced4f1daba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.결측치 및 이상치 처리\n",
    "X_train['Age'] = X_train['Age'].fillna(X_train['Age'].mean())\n",
    "X_test['Age'] = X_test['Age'].fillna(X_train['Age'].mean())\n",
    "\n",
    "X_train['Embarked'] = X_train['Embarked'].fillna(X_train['Embarked'].value_counts().idxmax())\n",
    "X_test['Embarked'] = X_test['Embarked'].fillna(X_train['Embarked'].value_counts().idxmax())\n",
    "\n",
    "X_train['Fare'] = X_train['Fare'].fillna(X_train['Fare'].mean())\n",
    "X_test['Fare'] = X_test['Fare'].fillna(X_train['Fare'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f61f6ed1-a6ef-4ded-b547-336e4e1d69d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #BK.이상치 처리\n",
    "\n",
    "# # sns.boxplot(data=X_train,y='Fare')\n",
    "# # print(X_train.sort_values(by='Fare',ascending=False))\n",
    "# # print(X_test.sort_values(by='Fare',ascending=False))\n",
    "\n",
    "# #Fare 특성과 다른 특성들과 상관분석 후 상관계수가 가장 높은 컬럼별 Fare 평균 값으로 이상치 대체\n",
    "# Fare_By_Pclass_train = pd.DataFrame(X_train.groupby(['Pclass'])['Fare'].mean())\n",
    "# X_train = pd.merge(X_train,Fare_By_Pclass_train,left_on='Pclass',right_on='Pclass',how='left')\n",
    "# X_train['Fare'] = X_train['Fare_x']*(X_train['Fare_x']!=X_train['Fare_x'].max()) + X_train['Fare_y']*(X_train['Fare_x']==X_train['Fare_x'].max())\n",
    "# X_train = X_train.drop(columns=['Fare_x','Fare_y'])\n",
    "\n",
    "# Fare_By_Pclass_test = pd.DataFrame(X_test.groupby(['Pclass'])['Fare'].mean())\n",
    "# X_test = pd.merge(X_test,Fare_By_Pclass_test,left_on='Pclass',right_on='Pclass',how='left')\n",
    "# X_test['Fare'] = X_test['Fare_x']*(X_test['Fare_x']!=X_test['Fare_x'].max()) + X_test['Fare_y']*(X_test['Fare_x']==X_test['Fare_x'].max())\n",
    "# X_test = X_test.drop(columns=['Fare_x','Fare_y'])\n",
    "\n",
    "# #사분위수를 활용한 이상치 처리\n",
    "# # Q1 = X_train['Fare'].quantile(0.25)\n",
    "# # Q3 = X_train['Fare'].quantile(0.75)\n",
    "# # IQR = Q3-Q1\n",
    "# # X_train.loc[(X_train['Fare']<Q1-1.5*IQR)|(X_train['Fare']>Q3+1.5*IQR),'Fare']=X_train['Fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4903de9-eff8-4996-9db1-eb2cc1c3d621",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #BK.기계학습을 활용한 이상치 처리\n",
    "# from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# X_train_Sub = X_train[['Age','Family','Fare']]\n",
    "# X_test_Sub = X_test[['Age','Family','Fare']]\n",
    "\n",
    "# model = IsolationForest(contamination=0.1, random_state=999)\n",
    "# model.fit(X_train_Sub)\n",
    "\n",
    "# X_train['outliers'] = model.predict(X_train_Sub)\n",
    "# X_test['outliers'] = model.predict(X_test_Sub)\n",
    "\n",
    "# X_train.loc[X_train['outliers']==-1,'Fare']=X_train['Fare'].mean()\n",
    "# X_train.loc[X_train['outliers']==-1,'Age']=X_train['Age'].mean()\n",
    "# X_train.loc[X_train['outliers']==-1,'Family']=X_train['Family'].mean()\n",
    "\n",
    "# X_test.loc[X_test['outliers']==-1,'Fare']=X_test['Fare'].mean()\n",
    "# X_test.loc[X_test['outliers']==-1,'Age']=X_test['Age'].mean()\n",
    "# X_test.loc[X_test['outliers']==-1,'Family']=X_test['Family'].mean()\n",
    "\n",
    "# X_train = X_train.drop(columns='outliers')\n",
    "# X_test = X_test.drop(columns='outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ba1c7a1-f97a-4111-86d8-61d290ae44af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #BK.Age컬럼 결측치 대체법\n",
    "# # 상관분석 실행\n",
    "# # print(X_train[['Age','Fare','Pclass','Family']].corr()['Age'])\n",
    "\n",
    "# # 상관계수가 높은 컬럼별로 group by해서 age 평균을 각 행의 결측치에 대체\n",
    "# Age_By_train = pd.DataFrame(X_train.groupby(['Pclass','Family'])['Age'].mean())\n",
    "# X_train = pd.merge(X_train, Age_By_train, left_on=['Pclass','Family'], right_on=['Pclass','Family'], how='left')\n",
    "# X_train['Age'] = X_train.apply(lambda row: row['Age_y'] if np.isnan(row['Age_x']) else row['Age_x'], axis=1)\n",
    "# X_train = X_train.drop(columns=['Age_x','Age_y'])\n",
    "\n",
    "# Age_By_test = pd.DataFrame(X_test.groupby(['Pclass','Family'])['Age'].mean())\n",
    "# X_test = pd.merge(X_test, Age_By_test, left_on=['Pclass','Family'], right_on=['Pclass','Family'], how='left')\n",
    "# X_test['Age'] = X_test.apply(lambda row: row['Age_y'] if np.isnan(row['Age_x']) else row['Age_x'], axis=1)\n",
    "# X_test = X_test.drop(columns=['Age_x','Age_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dfd2334-c95f-470f-9d4c-f574a9c3c62c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #BK.다중 대체법을 사용하여 결측치 대체\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# print(X_train[['Age','Family','Pclass','Fare']].corr()[['Age','Fare']])\n",
    "\n",
    "# model = RandomForestRegressor(random_state=999)\n",
    "\n",
    "# imputer = IterativeImputer(model, max_iter=10, random_state=999)\n",
    "\n",
    "# imputer_fit_train = imputer.fit(X_train[['Age','Pclass','Family']])\n",
    "# imputed_data_train = imputer_fit_train.transform(X_train[['Age','Pclass','Family']])\n",
    "# X_train['Age'] = imputed_data_train[:,0]\n",
    "# imputer_fit_train = imputer.fit(X_train[['Fare','Pclass']])\n",
    "# imputed_data_train = imputer_fit_train.transform(X_train[['Fare','Pclass']])\n",
    "# X_train['Fare'] = imputed_data_train[:,0]\n",
    "\n",
    "# imputer_fit_test = imputer.fit(X_test[['Age','Pclass','Family']])\n",
    "# imputed_data_test = imputer_fit_test.transform(X_test[['Age','Pclass','Family']])\n",
    "# X_test['Age'] = imputed_data_test[:,0]\n",
    "# imputer_fit_test = imputer.fit(X_test[['Fare','Pclass']])\n",
    "# imputed_data_test = imputer_fit_test.transform(X_test[['Fare','Pclass']])\n",
    "# X_test['Fare'] = imputed_data_test[:,0]\n",
    "\n",
    "# # print(X_test.shape[0],X_test['Fare'].isna().sum())\n",
    "\n",
    "# # test = pd.concat([pd.DataFrame(X_test['Fare']),pd.DataFrame(imputed_data_test[:,2])],axis=1)\n",
    "# # print((test['Fare']==test[0]).astype(int).sum())\n",
    "\n",
    "# # sns.countplot(data=X_test,x='Age')\n",
    "\n",
    "# print(X_train.isna().sum())\n",
    "# print(X_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab931bcb-e7bc-43b2-8bb2-bfb9ac913fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.인코딩\n",
    "X_train = pd.get_dummies(X_train,columns=['Sex','Embarked','Pclass'])\n",
    "X_test = pd.get_dummies(X_test,columns=['Sex','Embarked','Pclass'])\n",
    "\n",
    "X_train_enc = X_train.select_dtypes('bool').astype('int')\n",
    "X_test_enc = X_test.select_dtypes('bool').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ba9b552-0097-4537-b2dc-97eb93b69460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.스케일링\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_num = X_train.select_dtypes(exclude='bool').copy()\n",
    "X_test_num = X_test.select_dtypes(exclude='bool').copy()\n",
    "\n",
    "scale = StandardScaler().fit(X_train_num)\n",
    "\n",
    "X_train_std = scale.transform(X_train_num)\n",
    "X_test_std = scale.transform(X_test_num)\n",
    "\n",
    "X_train_std = pd.DataFrame(X_train_std,columns=['Age','Family','Fare'])\n",
    "X_test_std = pd.DataFrame(X_test_std,columns=['Age','Family','Fare'])\n",
    "\n",
    "X_train = pd.concat([X_train_std,X_train_enc],axis=1)\n",
    "X_test = pd.concat([X_test_std,X_test_enc],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b68bb37-40da-40ea-9878-999f5f55d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Family', 'Fare', 'Sex_female', 'Embarked_S', 'Pclass_3'], dtype='object')\n",
      "[0.16663802 0.21722868 0.08560329 0.39942311 0.02429701 0.10680989]\n"
     ]
    }
   ],
   "source": [
    "#7.특성 선택\n",
    "#RFE\n",
    "from sklearn.feature_selection import RFE, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "X = X_train.drop(columns=['Sex_male','Embarked_C','Pclass_1'])\n",
    "y = y_train\n",
    "test = X_test.drop(columns=['Sex_male','Embarked_C','Pclass_1'])\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "                    max_depth=7,\n",
    "                    n_estimators=200,\n",
    "                    max_features='sqrt',\n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf=1,\n",
    "                    random_state=999)\n",
    "\n",
    "rfe = RFE(model, n_features_to_select=6)  # 선택할 특성의 개수를 지정합니다.\n",
    "\n",
    "all_columns = X.columns\n",
    "\n",
    "# RFE 알고리즘을 사용하여 특성 선택을 수행합니다.\n",
    "rfe_fit = rfe.fit(X, y)\n",
    "\n",
    "# 선택된 특성들을 출력합니다.\n",
    "selected_mask = rfe_fit.support_\n",
    "print(all_columns[selected_mask])\n",
    "print(rfe_fit.estimator_.feature_importances_)\n",
    "\n",
    "X = X_train[['Age', 'Family', 'Fare', 'Sex_female', 'Embarked_S', 'Pclass_3']]\n",
    "test = X_test[['Age', 'Family', 'Fare', 'Sex_female', 'Embarked_S', 'Pclass_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a52aeae6-e105-443c-b446-644a2366113a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #BK.SelectKBest을 활용한 특성 선택 탐색\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# X = X_train.drop(columns=['Sex_male','Embarked_C','Pclass_1'])\n",
    "# y = y_train\n",
    "# test = X_test.drop(columns=['Sex_male','Embarked_C','Pclass_1'])\n",
    "\n",
    "# all_columns = X.columns\n",
    "\n",
    "# sel = SelectKBest(score_func=f_classif,k=6)\n",
    "# sel_fit = sel.fit(X,y)\n",
    "# X = sel_fit.transform(X)\n",
    "# test = sel_fit.transform(test)\n",
    "\n",
    "# selected_mask = sel_fit.get_support()\n",
    "# print(all_columns[selected_mask])\n",
    "# print(X)\n",
    "# print(test)\n",
    "\n",
    "# print(all_columns)\n",
    "# print(sel_fit.scores_.astype('int'))\n",
    "\n",
    "# X = X_train[['Age','Family','Sex_female','Embarked_S','Pclass_2','Pclass_3']]\n",
    "# y = y_train\n",
    "# test = X_test[['Age','Family','Sex_female','Embarked_S','Pclass_2','Pclass_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98e5a1ed-3260-4558-8b5e-8ba0aede84bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #BK.statsmodels.Logit() 통계적 분석 결과 표(p-value, coef 값)를 통한 특성 선택\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# X = X_train.drop(columns=['Sex_male','Embarked_C','Pclass_1'])\n",
    "# y = y_train\n",
    "# test = X_test.drop(columns=['Sex_male','Embarked_C','Pclass_1'])\n",
    "\n",
    "# model = sm.Logit(y,X)\n",
    "# model_fit = model.fit()\n",
    "# print(model_fit.summary())\n",
    "\n",
    "# X = X_train[['Age','Fare','Sex_female','Embarked_S','Pclass_2','Pclass_3']]\n",
    "# y = y_train\n",
    "# test = X_test[['Age','Fare','Sex_female','Embarked_S','Pclass_2','Pclass_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d0732ff-90f5-469a-b7de-850fe716c3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('max_depth', 11), ('min_samples_leaf', 4), ('min_samples_split', 2), ('n_estimators', 125)])\n"
     ]
    }
   ],
   "source": [
    "#8.하이퍼파라미터 최적화\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "        RandomForestClassifier(max_features='sqrt'),\n",
    "        {\n",
    "        'n_estimators': (10, 1000),   # 트리 개수\n",
    "        'max_depth': (3, 20),         # 트리 최대 깊이\n",
    "        'min_samples_split': (2, 10), # 분할을 위한 최소 샘플 수\n",
    "        'min_samples_leaf': (1, 10)   # 잎 노드에 필요한 최소 샘플 수\n",
    "        },\n",
    "        n_iter=20,   # 반복 횟수\n",
    "        cv=5,        # 교차 검증 폴드 수\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "opt.fit(X, y)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f872b0-fde5-4e5f-821f-dee3d309e7c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #BK.하이퍼파라미터 최적화(전체)\n",
    "# from skopt import BayesSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "# # 모델 정의\n",
    "# models = {\n",
    "#     'LogisticRegression': LogisticRegression(),\n",
    "#     'RandomForestClassifier': RandomForestClassifier(max_features='sqrt'),\n",
    "#     'XGBClassifier': XGBClassifier(),\n",
    "#     'LGBMClassifier': LGBMClassifier(verbose=-1)\n",
    "#     # 'CatBoostClassifier': CatBoostClassifier()\n",
    "# }\n",
    "\n",
    "# # 하이퍼파라미터 탐색 범위 지정\n",
    "# search_space = {\n",
    "#     'LogisticRegression': {\n",
    "#         'max_iter': (100, 1000),\n",
    "#         'C': (0.01, 100)\n",
    "#     },\n",
    "#     'RandomForestClassifier': {\n",
    "#         'n_estimators': (10, 1000),   # 트리 개수\n",
    "#         'max_depth': (3, 20),         # 트리 최대 깊이\n",
    "#         'min_samples_split': (2, 10), # 분할을 위한 최소 샘플 수\n",
    "#         'min_samples_leaf': (1, 10)   # 잎 노드에 필요한 최소 샘플 수\n",
    "#     },\n",
    "#     'XGBClassifier': {\n",
    "#         'n_estimators': (10, 1000),\n",
    "#         'max_depth': (3, 20),\n",
    "#         'min_child_weight': (1, 20),\n",
    "#         'learning_rate': (0.001, 1.0),\n",
    "#         'subsample': (0.1,1)\n",
    "#     },\n",
    "#     'LGBMClassifier': {\n",
    "#         'n_estimators': (10, 1000),\n",
    "#         'max_depth': (3, 20),\n",
    "#         'min_child_weight': (1, 20),\n",
    "#         'learning_rate': (0.001, 1.0),\n",
    "#         'num_leaves': (10,1000)\n",
    "#     }\n",
    "#     # 'CatBoostClassifier': {\n",
    "#     #     'iterations': (10, 1000),\n",
    "#     #     'depth': (3, 16),\n",
    "#     #     'learning_rate': (0.001, 1.0)\n",
    "#     # }\n",
    "# }\n",
    "\n",
    "# # 베이지안 최적화를 사용한 자동화된 하이퍼파라미터 튜닝\n",
    "# best_params = {}\n",
    "# for model_name, model in models.items():\n",
    "#     opt = BayesSearchCV(\n",
    "#         model,\n",
    "#         search_space[model_name],\n",
    "#         n_iter=10,   # 반복 횟수\n",
    "#         cv=5,        # 교차 검증 폴드 수\n",
    "#         scoring='accuracy'\n",
    "#     )\n",
    "#     opt.fit(X, y)\n",
    "#     best_params[model_name] = opt.best_params_\n",
    "\n",
    "# # 최적 하이퍼파라미터 출력\n",
    "# for model_name, params in best_params.items():\n",
    "#     print(model_name, \"best parameter:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f4a2184-da62-49b2-a10e-d876debd593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.모델 생성\n",
    "\n",
    "#LogisticRegression\n",
    "model_lr = LogisticRegression(\n",
    "                    max_iter=867,\n",
    "                    C=49.475,\n",
    "                    random_state=999)\n",
    "# model_lr.fit(X,y)\n",
    "# y_predict_lr = model_lr.predict(test)\n",
    "\n",
    "#RandomForestClassifier\n",
    "model_rf = RandomForestClassifier(\n",
    "                    max_features='sqrt',\n",
    "                    max_depth=7,\n",
    "                    n_estimators=200,\n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf=1,\n",
    "                    random_state=999)\n",
    "# model_rf.fit(X,y)\n",
    "# y_predict_rf = model_rf.predict(test)\n",
    "\n",
    "#XGBClassifier\n",
    "model_xgb = XGBClassifier(\n",
    "                    max_depth=14,\n",
    "                    n_estimators=720,\n",
    "                    min_child_weight=17,\n",
    "                    learning_rate=0.264,\n",
    "                    subsample=0.971,\n",
    "                    random_state=999)\n",
    "# model_xgb.fit(X,y)\n",
    "# y_predict_xgb = model_xgb.predict(test)\n",
    "\n",
    "#LGBMClassifier\n",
    "model_lgbm = LGBMClassifier(\n",
    "                    max_depth=19,\n",
    "                    n_estimators=36,\n",
    "                    min_child_weight=11,\n",
    "                    learning_rate=0.972,\n",
    "                    num_leaves=287,\n",
    "                    verbose=-1,\n",
    "                    random_state=999)\n",
    "# model_lgbm.fit(X,y)\n",
    "# y_predict_lgbm = model_lgbm.predict(test)\n",
    "\n",
    "#CatBoostClassifier\n",
    "# model_cat = CatBoostClassifier(\n",
    "#                     depth=2,\n",
    "#                     iterations=100,\n",
    "#                     learning_rate=0.2)\n",
    "# model_cat.fit(X,y,verbose=False)\n",
    "# y_predict_cat = np.array(model_cat.predict(test)).flatten() #array in array 형태를 array로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ca3a8f3-e963-4f64-a9e4-328c46387ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808\n",
      "0.825\n",
      "0.822\n",
      "0.828\n"
     ]
    }
   ],
   "source": [
    "#10.모델 학습 및 평가\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "kfold = KFold(n_splits=5,shuffle=True,random_state=999)\n",
    "print(cross_validate(model_lr, X, y, cv=kfold, scoring='accuracy')['test_score'].mean().round(3))\n",
    "print(cross_validate(model_rf, X, y, cv=kfold, scoring='accuracy')['test_score'].mean().round(3))\n",
    "print(cross_validate(model_xgb, X, y, cv=kfold, scoring='accuracy')['test_score'].mean().round(3))\n",
    "print(cross_validate(model_lgbm, X, y, cv=kfold, scoring='accuracy')['test_score'].mean().round(3))\n",
    "# print(cross_validate(model_cat, X, y, cv=kfold, scoring='accuracy',fit_params={\"verbose\":False})['test_score'].mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0561a01-a5b7-4c4b-8c2e-11c909c8a712",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #BK.홀드아웃 기법\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X1, X2, y1, y2 = train_test_split(X,y,train_size=0.8,test_size=0.2,random_state=999)\n",
    "\n",
    "# print(accuracy_score(y2,y_predict_lr).round(3))\n",
    "# print(f1_score(y2,y_predict_lr).round(3))\n",
    "# probas_lr = model_lr.predict_proba(X2)\n",
    "# print(roc_auc_score(y2,probas_lr[:,1]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a67961c-afea-4ed0-a93c-94144df2e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.실 데이터 예측 및 제출\n",
    "model_rf = RandomForestClassifier(\n",
    "                    max_depth=7,\n",
    "                    n_estimators=200,\n",
    "                    max_features='sqrt',\n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf=1,\n",
    "                    random_state=999)\n",
    "model_rf.fit(X,y)\n",
    "y_predict_rf = model_rf.predict(test)\n",
    "\n",
    "'''\n",
    "#Best Model\n",
    "model_rf = RandomForestClassifier(\n",
    "                    max_depth=7,\n",
    "                    n_estimators=200,\n",
    "                    max_features='sqrt',\n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf=1,\n",
    "                    random_state=999)\n",
    "model_rf.fit(X,y)\n",
    "y_predict_rf = model_rf.predict(test)\n",
    "'''\n",
    "\n",
    "obj = {'PassengerId':PassengerId,'Survived':y_predict_rf}\n",
    "result = pd.DataFrame(obj)\n",
    "result.to_csv('result/result.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
